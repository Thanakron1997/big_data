{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Plot confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns \n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = [\"positive\",\"negative\",\"normal\"]\n",
    "# figure object\n",
    "plt.figure(figsize=(7, 7))\n",
    "cm = [[1365024.,39645.,24020.],[54710.,420324.,21444.],[ 119578.,62023.,44302.]]\n",
    "# plot confusion matrix\n",
    "sns.heatmap(cm,\n",
    "            cmap='viridis',\n",
    "            annot=True,fmt='0',\n",
    "            cbar=False, \n",
    "            xticklabels=labels, \n",
    "            yticklabels=labels)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check word in summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re \n",
    "import spacy  # For preprocessing\n",
    "df = pd.read_csv(\"data_set_tiktok_update_2_result.csv\",low_memory=False)\n",
    "df.info()\n",
    "df2 = df[['playCount','commentCount','diggCount','shareCount','summary','object']]\n",
    "def clean_1(text):\n",
    "    try:\n",
    "        return text.replace(\"In the video,\", \"\")\n",
    "    except:\n",
    "        return text\n",
    "def clean_2(text):\n",
    "    try:\n",
    "        return text.replace(\"The video\", \"\")\n",
    "    except:\n",
    "        return text\n",
    "df2['summary'] = df2['summary'].apply(clean_1)\n",
    "df2['object'] = df2['object'].apply(clean_2)\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_lg\",disable=['ner', 'parser'])\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    removelst = ['cq', 'rg', 'bb']\n",
    "    txt = [w for w in txt if w not in removelst]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df2['summary'])\n",
    "df2['clean'] = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, n_process=-1, batch_size=5000)]\n",
    "object_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df2['object'])\n",
    "df2['object_clen'] = [cleaning(doc) for doc in nlp.pipe(object_cleaning, n_process=-1, batch_size=5000)]\n",
    "del nlp\n",
    "df2 = df2.dropna().reset_index(drop=True)\n",
    "# df2['token'] = df2['clean'].apply(lambda x: x.split())\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['object_clen_split'] = df2['object_clen'].apply(lambda x: x.split())\n",
    "del_lst = ['video','appear','show','hold','possibly','engage','engage','wear','scene','black','suggest','likely','casual','content','close','talk','setting','provide','person']\n",
    "def del_word(x):\n",
    "    return list(set([text for text in x if text not in del_lst]))\n",
    "df2['object_clen_split'] = df2['object_clen_split'].apply(del_word)\n",
    "df_exploded = df2['object_clen_split'].explode('object_clen_split')\n",
    "x = df_exploded.value_counts()\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.to_csv(\"video_clean.csv\",index=False)\n",
    "# spark_data = spark.createDataFrame(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### summary tiktok video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfx = pd.read_csv(\"data_video_predict.csv\",low_memory=False)\n",
    "dfx.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0.0 -> 1.0 positive \\\n",
    "1.0 -> -1.0 negative \\\n",
    "2.0 -> 0.0 normal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx2 = dfx[['playCount','commentCount','diggCount','shareCount','summary','prediction']]\n",
    "def return_val(x):\n",
    "    if x == 0.0:\n",
    "        return \"positive\"\n",
    "    elif x == 1.0:\n",
    "        return \"negative\"\n",
    "    elif x == 2.0:\n",
    "        return \"normal\"\n",
    "dfx2['prediction_val'] = dfx2['prediction'].apply(return_val)\n",
    "dfx2['prediction_val'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pos = dfx2[dfx2['prediction_val']==\"positive\"]\n",
    "df_neg = dfx2[dfx2['prediction_val']==\"negative\"]\n",
    "print(df_pos.describe().round(decimals=2))\n",
    "print(df_neg.describe().round(decimals=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check similarity "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dfx = pd.read_csv(\"data_set_tiktok_update_2_result.csv\",low_memory=False)\n",
    "dfx = dfx[['webVideoUrl','summary','object']]\n",
    "dfx.info()\n",
    "dfxx = pd.read_csv(\"validate-videosummary-f15.csv\",low_memory=False)\n",
    "dfxx.info()\n",
    "df_merge = pd.merge(dfxx,dfx,on=\"webVideoUrl\",how=\"left\")\n",
    "df_merge.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy \n",
    "def clean_1(text):\n",
    "    try:\n",
    "        return text.replace(\"In the video,\", \"\")\n",
    "    except:\n",
    "        return text\n",
    "def clean_2(text):\n",
    "    try:\n",
    "        return text.replace(\"The video shows\", \"\")\n",
    "    except:\n",
    "        return text\n",
    "df_merge['summary'] = df_merge['summary'].apply(clean_1)\n",
    "df_merge['summary_from_human'] = df_merge['summary_from_human'].apply(clean_2)\n",
    "\n",
    "spacy.prefer_gpu()\n",
    "nlp = spacy.load(\"en_core_web_lg\",disable=['ner', 'parser'])\n",
    "def cleaning(doc):\n",
    "    # Lemmatizes and removes stopwords\n",
    "    # doc needs to be a spacy Doc object\n",
    "    txt = [token.lemma_ for token in doc if not token.is_stop]\n",
    "    removelst = ['cq', 'rg', 'bb']\n",
    "    txt = [w for w in txt if w not in removelst]\n",
    "    if len(txt) > 2:\n",
    "        return ' '.join(txt)\n",
    "brief_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df_merge['summary'])\n",
    "df_merge['summary_clean'] = [cleaning(doc) for doc in nlp.pipe(brief_cleaning, n_process=-1, batch_size=5000)]\n",
    "summary_from_human_cleaning = (re.sub(\"[^A-Za-z']+\", ' ', str(row)).lower() for row in df_merge['summary_from_human'])\n",
    "df_merge['summary_human_clen'] = [cleaning(doc) for doc in nlp.pipe(summary_from_human_cleaning, n_process=-1, batch_size=5000)]\n",
    "\n",
    "def similarity_check(row):\n",
    "    text1 = nlp(row['summary_human_clen'])\n",
    "    text2 = nlp(row['summary_clean'])\n",
    "    return text2.similarity(text1)\n",
    "df_merge['similarity'] = df_merge.apply(similarity_check,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merge['similarity'].describe()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
